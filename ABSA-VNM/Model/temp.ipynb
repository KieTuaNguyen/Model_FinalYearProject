{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xml_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "            content = file.read()\n",
    "            \n",
    "        # Remove any invisible characters\n",
    "        content = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', content)\n",
    "        \n",
    "        # Parse the cleaned content\n",
    "        root = ET.fromstring(content)\n",
    "        return root\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"XML Parse Error: {e}\")\n",
    "        print(f\"Error occurred at line {e.position[0]}, column {e.position[1]}\")\n",
    "        \n",
    "        # Print the problematic line\n",
    "        lines = content.split('\\n')\n",
    "        if e.position[0] <= len(lines):\n",
    "            print(f\"Problematic line: {lines[e.position[0] - 1].strip()}\")\n",
    "            print(f\"                  {' ' * (e.position[1] - 1)}^\")\n",
    "        return None\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)])\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids)\n",
    "        probs = out.logits.softmax(dim=-1).tolist()[0]\n",
    "    return {\n",
    "        \"negative\": probs[0],\n",
    "        \"positive\": probs[1],\n",
    "        \"neutral\": probs[2]\n",
    "    }\n",
    "\n",
    "def get_dominant_sentiment(sentiment_dict):\n",
    "    return max(sentiment_dict, key=sentiment_dict.get)\n",
    "\n",
    "def train_classifier(xml_root):\n",
    "    product_terms = []\n",
    "    service_terms = []\n",
    "    \n",
    "    for sentence in xml_root.findall('.//sentence'):\n",
    "        terms = [term.attrib['term'] for term in sentence.find('aspectTerms')]\n",
    "        categories = [cat.attrib['category'] for cat in sentence.find('aspectCategories')]\n",
    "        \n",
    "        if \"Về Sản Phẩm\" in categories:\n",
    "            product_terms.extend(terms)\n",
    "        if \"Về Dịch Vụ\" in categories:\n",
    "            service_terms.extend(terms)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(product_terms + service_terms)\n",
    "    \n",
    "    product_vector = np.mean(tfidf_matrix[:len(product_terms)].toarray(), axis=0)\n",
    "    service_vector = np.mean(tfidf_matrix[len(product_terms):].toarray(), axis=0)\n",
    "    \n",
    "    return vectorizer, product_vector, service_vector\n",
    "\n",
    "def classify_term(term, vectorizer, product_vector, service_vector):\n",
    "    term_vector = vectorizer.transform([term]).toarray()\n",
    "    product_similarity = cosine_similarity(term_vector, product_vector.reshape(1, -1))[0][0]\n",
    "    service_similarity = cosine_similarity(term_vector, service_vector.reshape(1, -1))[0][0]\n",
    "    \n",
    "    if product_similarity > service_similarity:\n",
    "        return \"Về Sản Phẩm\"\n",
    "    else:\n",
    "        return \"Về Dịch Vụ\"\n",
    "\n",
    "def process_sentence(sentence, vectorizer, product_vector, service_vector):\n",
    "    text = sentence.find('text').text\n",
    "    aspect_terms = sentence.find('aspectTerms')\n",
    "    aspect_categories = sentence.find('aspectCategories')\n",
    "    \n",
    "    for term in aspect_terms:\n",
    "        start = max(0, text.find(term.attrib['term']) - 20)\n",
    "        end = min(len(text), text.find(term.attrib['term']) + len(term.attrib['term']) + 20)\n",
    "        context = text[start:end]\n",
    "        \n",
    "        sentiment = analyze_sentiment(context)\n",
    "        dominant_sentiment = get_dominant_sentiment(sentiment)\n",
    "        dominant_score = sentiment[dominant_sentiment]\n",
    "        \n",
    "        term.set('polarity', dominant_sentiment)\n",
    "        term.set('dominant_score', f\"{dominant_score:.4f}\")\n",
    "        \n",
    "        category = classify_term(term.attrib['term'], vectorizer, product_vector, service_vector)\n",
    "        term.set('category', category)\n",
    "    \n",
    "    # Clear existing categories and add new ones based on term classifications\n",
    "    aspect_categories.clear()\n",
    "    categories = set(term.get('category') for term in aspect_terms)\n",
    "    for category in categories:\n",
    "        ET.SubElement(aspect_categories, 'aspectCategory', {'category': category})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kietd\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved in 'processed.xml'.\n"
     ]
    }
   ],
   "source": [
    "# Load and process the XML data\n",
    "xml_root = load_xml_data('manual_label.xml')\n",
    "\n",
    "if xml_root is None:\n",
    "    print(\"Failed to load XML. Please fix the XML file and try again.\")\n",
    "else:\n",
    "    # Train the classifier\n",
    "    vectorizer, product_vector, service_vector = train_classifier(xml_root)\n",
    "\n",
    "    # Process sentences\n",
    "    for sentence in xml_root.findall('.//sentence'):\n",
    "        process_sentence(sentence, vectorizer, product_vector, service_vector)\n",
    "\n",
    "    # Write the processed XML to a file\n",
    "    tree = ET.ElementTree(xml_root)\n",
    "    tree.write('processed.xml', encoding='utf-8-sig', xml_declaration=True)\n",
    "\n",
    "    print(\"Processing complete. Results saved in 'processed.xml'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
