{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import warnings\n",
    "import re\n",
    "import emoji\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Import and Initial Processing\n",
    "\n",
    "The data import and initial processing stage focuses on loading the feedback data from a CSV file and performing preliminary cleaning. It reads the data into a pandas DataFrame, prints the initial number of entries, removes duplicate entries based on 'GeneralFeedbackID' while keeping the first occurrence, converts the 'Content' column to string type, and prints the number of unique entries. This step ensures that the dataset is prepared for more detailed processing in the following stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 457415\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/feedback_detail.csv', encoding='utf-8-sig')\n",
    "print('Number of records:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 21880\n"
     ]
    }
   ],
   "source": [
    "df = df[['GeneralFeedbackID', 'Content']].drop_duplicates(subset='GeneralFeedbackID', keep='first')\n",
    "df['Content'] = df['Content'].astype(str)\n",
    "print('Number of records:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "The tokenization section breaks down the feedback content into smaller, meaningful segments. It defines two key functions: 'clean_and_tokenize' which splits text by commas and periods while cleaning each segment, and 'tokenize_content' which applies this tokenization to each row of the dataframe. The process then explodes the tokenized content into separate rows and adds an 'ID' column to number the segments within each 'GeneralFeedbackID'. This step is crucial for transforming the raw feedback text into a format more suitable for analysis.\n",
    "\n",
    "Currently we have the data in the format of MasterID, FullText\n",
    "\n",
    "| MasterID | FullText |\n",
    "|----------|----------|\n",
    "| 1        | This is a sentence, and this is another sentence. This is the third sentence. |\n",
    "\n",
    "We will tokenize the sentences and reconstruct the data to the format of MasterID, ID, Text\n",
    "\n",
    "| MasterID | ID | Text |\n",
    "|----------|----|------|\n",
    "| 1        | 0  | this is a sentence |\n",
    "| 1        | 1  | and this is another sentence |\n",
    "| 1        | 2  | this is the third sentence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    text = text.strip('\"')\n",
    "    segments = re.split(r'[,.]', text)\n",
    "    cleaned_segments = [segment.strip().rstrip(',') for segment in segments if segment.strip()]\n",
    "    return cleaned_segments\n",
    "\n",
    "def tokenize_content(row):\n",
    "    try:\n",
    "        segments = clean_and_tokenize(row['Content'])\n",
    "        return pd.Series({\n",
    "            'GeneralFeedbackID': row['GeneralFeedbackID'],\n",
    "            'Content': segments\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row['GeneralFeedbackID']}: {e}\")\n",
    "        return pd.Series({\n",
    "            'GeneralFeedbackID': row['GeneralFeedbackID'],\n",
    "            'Content': [row['Content']] \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenized = df.apply(tokenize_content, axis=1)\n",
    "df_exploded = df_tokenized.explode('Content').reset_index(drop=True)\n",
    "df_exploded['ID'] = df_exploded.groupby('GeneralFeedbackID').cumcount()\n",
    "df = df_exploded[['GeneralFeedbackID', 'ID', 'Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 37699\n"
     ]
    }
   ],
   "source": [
    "print('Number of records:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning and Filtering\n",
    "\n",
    "This comprehensive section focuses on cleaning the text data and filtering out meaningless content. It defines several functions to remove unwanted elements such as URLs, HTML tags, special characters, hashtags, and phone numbers. The 'is_meaningful' function checks if the text is meaningful based on length, repetition, and pattern criteria. The 'clean_text' function applies all cleaning operations to the text. These functions are then applied to the 'Content' column, removing rows with non-meaningful content and any remaining empty rows. This step ensures that the final dataset contains only clean, meaningful feedback entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s\\u00C0-\\u1EF9.,!?]')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "def remove_phone_numbers(text):\n",
    "    return re.sub(r'\\b(?:\\+?84|0)(?:\\d{9,10})\\b', '[PHONE]', text)\n",
    "\n",
    "def is_meaningful(text, min_length=2, max_length=200, max_repetition_ratio=0.5, max_consonant_streak=5):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    \n",
    "    if len(cleaned_text) < min_length or len(cleaned_text) > max_length:\n",
    "        return False\n",
    "    \n",
    "    if re.search(r'[bcdfghjklmnpqrstvwxyz]{' + str(max_consonant_streak) + ',}', cleaned_text):\n",
    "        return False\n",
    "    \n",
    "    char_counts = Counter(cleaned_text)\n",
    "    most_common_char_count = char_counts.most_common(1)[0][1]\n",
    "    repetition_ratio = most_common_char_count / len(cleaned_text)\n",
    "    \n",
    "    if repetition_ratio > max_repetition_ratio:\n",
    "        return False\n",
    "    \n",
    "    for pattern_length in range(2, 6):\n",
    "        for i in range(len(cleaned_text) - pattern_length * 2):\n",
    "            pattern = cleaned_text[i:i+pattern_length]\n",
    "            if pattern == cleaned_text[i+pattern_length:i+pattern_length*2]:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_phone_numbers(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].astype(str)\n",
    "df['Content'] = df['Content'].apply(clean_text)\n",
    "df['is_meaningful'] = df['Content'].apply(is_meaningful)\n",
    "df = df[df['is_meaningful']]\n",
    "df = df.drop(columns=['is_meaningful'])\n",
    "df = df[df['Content'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 25513\n"
     ]
    }
   ],
   "source": [
    "print('Number of records:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a subset from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4400\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(15)\n",
    "df = df.sample(n=4400, random_state=15)\n",
    "print('Number of records:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('data/feedback_subset.json', orient='records', indent=2, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
